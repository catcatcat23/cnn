{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): AdaptiveMaxPool2d(output_size=(4, 4))\n",
      "  )\n",
      "  (classify): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 定义卷积层\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1\n",
    "            nn.Conv2d(1, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            # Pool1\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv2\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            # AdaptiveMaxPool2d\n",
    "            nn.AdaptiveMaxPool2d(output_size=(4, 4)),\n",
    "        )\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.classify = nn.Sequential(\n",
    "            # FC1\n",
    "            nn.Linear(50 * 4 * 4, 500),\n",
    "            nn.ReLU(),\n",
    "            # FC2\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过卷积层\n",
    "        x = self.features(x)\n",
    "        # 展平为一维向量\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 通过全连接层\n",
    "        x = self.classify(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnx\n",
    "\n",
    "导出模型为 ONNX 格式有许多重要的作用，主要体现在以下几个方面：\n",
    "\n",
    "1. **跨平台兼容性**：\n",
    "   - ONNX 支持多种深度学习框架（如 TensorFlow、PyTorch、Caffe2、MXNet 等）之间的模型互操作性。这意味着您可以在一个框架中训练模型，然后在另一个框架中部署它，从而实现模型的迁移和复用。\n",
    "\n",
    "2. **硬件加速**：\n",
    "   - ONNX 支持多种硬件加速库，如 Intel 的 OpenVINO、NVIDIA 的 TensorRT 等。这些库可以针对特定硬件（如 CPU、GPU、FPGA 等）优化模型的推理速度和性能。\n",
    "\n",
    "3. **简化部署**：\n",
    "   - 将模型导出为 ONNX 格式可以简化从训练环境到生产环境的部署流程。ONNX 模型可以在多种平台上运行，包括移动设备、边缘计算设备和服务器。\n",
    "\n",
    "4. **模型验证**：\n",
    "   - 导出为 ONNX 格式可以帮助验证模型的正确性和一致性。ONNX 提供了工具来检查模型的有效性，并且可以在不同的框架中测试模型的一致性。\n",
    "\n",
    "5. **可视化和调试**：\n",
    "   - ONNX 模型可以使用工具（如 Netron）进行可视化，这对于理解模型结构、调试模型和优化模型都非常有用。\n",
    "\n",
    "6. **统一接口**：\n",
    "   - ONNX 提供了一种标准化的方式来表示机器学习模型，使得不同框架之间的接口更加统一，降低了跨框架使用的难度。\n",
    "\n",
    "7. **模型优化**：\n",
    "   - ONNX 支持多种优化技术，如量化、剪枝等，这些技术可以帮助减少模型大小、提高推理速度和降低功耗。\n",
    "\n",
    "8. **多框架支持**：\n",
    "   - ONNX 社区持续增长，越来越多的框架和工具开始支持 ONNX 格式，这为开发者提供了更多的选择和灵活性。\n",
    "\n",
    "### 示例应用\n",
    "\n",
    "- **移动应用开发**：在移动应用中部署深度学习模型时，通常会将模型导出为 ONNX 格式，然后使用支持 ONNX 的移动框架（如 CoreML 或 MLKit）进行推理。\n",
    "  \n",
    "- **边缘计算**：在边缘设备上部署模型时，ONNX 格式可以帮助优化模型在有限资源下的性能。\n",
    "\n",
    "- **云计算**：在云服务中，ONNX 可以帮助实现模型的快速部署和优化，提高服务的响应速度。\n",
    "\n",
    "导出模型为 ONNX 格式是一个非常有用的步骤，特别是在将模型从研究阶段过渡到生产阶段时。通过 ONNX，您可以更好地管理和优化模型的整个生命周期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model exported to lenet.onnx\n"
     ]
    }
   ],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 创建示例输入\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# 导出模型为 ONNX 格式\n",
    "output_file = \"lenet.onnx\"\n",
    "torch.onnx.export(model, dummy_input, output_file,\n",
    "                  export_params=True,        # 存储训练过的参数\n",
    "                  opset_version=10,         # ONNX 版本\n",
    "                  do_constant_folding=True, # 是否执行常量折叠优化\n",
    "                  input_names=['input'],    # 输入名称\n",
    "                  output_names=['output'],  # 输出名称\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}} # 批次大小动态\n",
    "                  )\n",
    "\n",
    "print(f\"ONNX model exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime output:\n",
      "[array([[ 0.07235464,  0.04472331, -0.14365256,  0.1352329 , -0.01156872,\n",
      "         0.01551914,  0.06356695, -0.08058672, -0.04503734,  0.01938414]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# 加载 ONNX 运行时\n",
    "ort_session = onnxruntime.InferenceSession(output_file)\n",
    "\n",
    "# 创建输入数据\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "input_data = np.random.randn(1, 1, 28, 28).astype(np.float32)\n",
    "\n",
    "# 运行模型\n",
    "ort_outputs = ort_session.run(None, {input_name: input_data})\n",
    "\n",
    "print(\"ONNX Runtime output:\")\n",
    "print(ort_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## netron可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'lenet.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "\n",
    "# 使用 netron 查看 ONNX 模型\n",
    "netron.start(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
